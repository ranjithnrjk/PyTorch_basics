{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e2104f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0594e712",
   "metadata": {},
   "source": [
    "# 1. Automatic Differentiation with `torch.autograd`\n",
    "\n",
    "Before proceeding autograd, will understand the basic terms:\n",
    "\n",
    "- **Forward Propagation**:\n",
    "  - Computes the model's output by passing the input data through the network layers. It is often called Forward pass.\n",
    "\n",
    "- **Backward Propagation**:\n",
    "  - Calculates the gradients of the loss with respect to the model's parameters using the chain rule, enabling parameter updates to minimize the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486315cc",
   "metadata": {},
   "source": [
    "### 1.1 ```torch.autograd```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9001c2b5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- We create two tensors `x` and `y` with `requires_grad=True`, indicating that we want to compute gradients for these tensors.\n",
    "\n",
    "\n",
    "\n",
    "- We perform simple operations on `x` and `y` to obtain `z`.\n",
    "\n",
    "- Computing Gradients:\n",
    "We call `z.backward()` to compute the gradients of `z` with respect to `x` and `y`. The gradients are stored in the `grad` attribute of each tensor.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In the following example:\n",
    "\n",
    "\n",
    "- The operation is $ z = x \\cdot y + y^2 $.\n",
    "- The partial derivative of $ z $ with respect to $ x $ is $ \\frac{\\partial z}{\\partial x} = y $.\n",
    "- The partial derivative of $ z $ with respect to $ y $ is $ \\frac{\\partial z}{\\partial y} = x + 2y $.\n",
    "\n",
    "Given $ x = 2.0 $ and $ y = 3.0 $:\n",
    "\n",
    "- The gradient of $ z $ w.r.t. $ x $ is $ 3.0 $.\n",
    "- The gradient of $ z $ w.r.t. $ y $ is $ 2.0 + 2 \\cdot 3.0 = 8.0 $.\n",
    "\n",
    "Tensors that require gradients will have their operations tracked by PyTorch's autograd engine, enabling the computation of gradients during backpropagation.\n",
    "\n",
    "\n",
    "<img src=https://learnopencv.com/wp-content/uploads/2024/07/Autograd-Computation-Graph-2-2.png height = 500>\n",
    "\n",
    "\n",
    "The automatic differentiation provided by `torch.autograd` simplifies this process by handling the complex chain rule calculations needed for backpropagation through the entire network.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For $\\frac{\\partial z}{\\partial x}$:\n",
    "\n",
    "$$\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial p} \\frac{\\partial p}{\\partial x} + \\frac{\\partial z}{\\partial q} \\frac{\\partial q}{\\partial x} = 1 \\cdot y + 1 \\cdot 0 = y$$\n",
    "\n",
    "For $\\frac{\\partial z}{\\partial y}$:\n",
    "\n",
    "$$\\frac{\\partial z}{\\partial y} = \\frac{\\partial z}{\\partial p} \\frac{\\partial p}{\\partial y} + \\frac{\\partial z}{\\partial q} \\frac{\\partial q}{\\partial y} = 1 \\cdot x + 1 \\cdot 2y = x + 2y$$\n",
    "\n",
    "These equations correspond to the chain rule calculations happening behind the scenes, demonstrating how PyTorch's autograd system computes gradients through the computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00238b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[20., 24., 28.],\n",
      "        [30., 35., 40.]], grad_fn=<AddBackward0>)\n",
      "tensor(177., grad_fn=<SumBackward0>)\n",
      "Gradient of x: tensor([9., 9., 9.])\n",
      "Gradient of y: tensor([[30.],\n",
      "        [36.]])\n",
      "Gradient of z: tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "Result of the operation: z = tensor([[20., 24., 28.],\n",
      "        [30., 35., 40.]])\n"
     ]
    }
   ],
   "source": [
    "# Create tensors with requires_grad=True\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = torch.tensor([[4.0], [5.0]], requires_grad=True)\n",
    "\n",
    "# Perform some operations\n",
    "z = x * y + y**2\n",
    "\n",
    "z.retain_grad()  # Retain gradients for z\n",
    "\n",
    "# Compute the gradients\n",
    "z_sum = z.sum()\n",
    "z_sum.backward()\n",
    "\n",
    "print(z)\n",
    "print(z_sum)\n",
    "print(f\"Gradient of x: {x.grad}\")\n",
    "print(f\"Gradient of y: {y.grad}\")\n",
    "print(f\"Gradient of z: {z.grad}\")\n",
    "print(f\"Result of the operation: z = {z.detach()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06339171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2., 3.], requires_grad=True),\n",
       " tensor([[4.],\n",
       "         [5.]], requires_grad=True),\n",
       " torch.Size([3]),\n",
       " torch.Size([2, 1]),\n",
       " torch.Size([2, 3]),\n",
       " tensor([[ 4.,  8., 12.],\n",
       "         [ 5., 10., 15.]], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y, x.shape, y.shape, (x * y).shape, x*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1576a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[16.],\n",
       "        [25.]], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7014e96",
   "metadata": {},
   "source": [
    "### 1.2 Gradient Computation Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91a1251",
   "metadata": {},
   "source": [
    "## 1.2. Gradient Computation Graph\n",
    "\n",
    "\n",
    "A computation graph is a visual representation of the sequence of operations performed on tensors in a neural network, showing how each operation contributes to the final result. It is crucial for understanding and debugging the flow of data and gradients in deep learning models.\n",
    "\n",
    "[torchviz](https://github.com/szagoruyko/pytorchviz) is a tool used to visualize the computation graph of any PyTorch model.\n",
    "\n",
    "\n",
    "<img src=https://learnopencv.com/wp-content/uploads/2024/07/Autograd-Operators-Graph-1-1.png height = 500 >\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26da932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
